{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dipendenze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install datasets\n",
    "%pip install transformers[torch]\n",
    "%pip install tokenizers\n",
    "%pip install evaluate\n",
    "%pip install rouge_score\n",
    "%pip install sentencepiece\n",
    "%pip install huggingface_hub\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generazione risposta del modello con singola stringa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Questa funzione si occupa di ottenere le entità estratte dal modello e di organizzarle in un dizionario\n",
    "\n",
    "def extract_entities_with_categories_from_prediction(sentence):\n",
    "  pattern = re.compile(r'(\\w+):\\s*([^;]+)')\n",
    "  matches = pattern.findall(sentence)\n",
    "  result_dict = {}\n",
    "  for match in matches:\n",
    "      key = match[0].strip()\n",
    "      values = match[1].strip()\n",
    "      if(key in result_dict.keys()):\n",
    "        result_dict[key].append(values)\n",
    "      else:\n",
    "        result_dict[key] = []\n",
    "        result_dict[key].append(values)\n",
    "  return result_dict\n",
    "\n",
    "# Funzione per generare la risposta ad una domanda, restituisce un dizionario con le entità estratte dal modello e i rispettivi tipi\n",
    "def generate_question(question,path_model):\n",
    "  prefix = \"NER: \"\n",
    "  last_checkpoint = path_model\n",
    "  finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(last_checkpoint, device_map=\"auto\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(last_checkpoint)\n",
    "  inputs = f\"{prefix}: {question}\"\n",
    "  inputs = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "  outputs = finetuned_model.generate(**inputs,max_new_tokens= 80,do_sample = False)\n",
    "  answer = tokenizer.decode(outputs[0],skip_special_tokens = True)\n",
    "  return extract_entities_with_categories_from_prediction(answer)\n",
    "\n",
    "path_model = \"Andro9669/flan-t5-ner\"\n",
    "question = \"An entity-oriented approach to restricted-domain parsing is proposed \"\n",
    "print(generate_question(question, path_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generazione risposte in file JSON del test set di SCIERC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Path del checkpoint del modello fine tunato, del file json dei risultati e del dataset\n",
    "path_model = \"Andro9669/flan-t5-ner\"\n",
    "path_result = \"/content/drive/MyDrive/Tesi/result/result_flan_large_no_diff_10.json\"\n",
    "path_dataset = \"/content/drive/MyDrive/Tesi/dataset/scierc_test_no_diff.json\"\n",
    "\n",
    "\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(path_model,device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_model)\n",
    "\n",
    "prefix = \"NER: \"\n",
    "# Per ogni domanda del dataset, genero la risposta e la salvo nel file json dei risultati\n",
    "with open(path_dataset) as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        question = data[\"input\"]\n",
    "        inputs = f\"{prefix}: {question}\"\n",
    "        inputs = tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = finetuned_model.generate(**inputs,max_new_tokens= 80,do_sample = False)\n",
    "        answer = tokenizer.decode(outputs[0],skip_special_tokens = True)\n",
    "        print(answer)\n",
    "        with open(path_result, 'a') as outfile:\n",
    "          json_object = {\"prediction\": answer}\n",
    "          json.dump(json_object, outfile)\n",
    "          outfile.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
