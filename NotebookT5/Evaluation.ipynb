{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni che sono necessarie per la corretta estrazione e organizzazione delle risposte del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Questa funzione si occupa di ottenere le entità estratte dal modello e di organizzarle in un dizionario\n",
    "\n",
    "def extract_entities_with_categories_from_prediction(sentence):\n",
    "  pattern = re.compile(r'(\\w+):\\s*([^;]+)')\n",
    "  matches = pattern.findall(sentence)\n",
    "  result_dict = {}\n",
    "  for match in matches:\n",
    "      key = match[0].strip()\n",
    "      values = match[1].strip()\n",
    "      if(key in result_dict.keys()):\n",
    "        result_dict[key].append(values)\n",
    "      else:\n",
    "        result_dict[key] = []\n",
    "        result_dict[key].append(values)\n",
    "  return result_dict\n",
    "\n",
    "# Questa funzione si occupa di organizzare le entità che sono presenti nel dataset in un dizionario\n",
    "def extract_entities_with_categories_from_data(data):\n",
    "  keys = [\"Metric\", \"Method\", \"Material\", \"Task\", \"OtherScientificTerm\", \"Generic\"]\n",
    "  result_dict = {}\n",
    "  for key in keys:\n",
    "    sentences = data[key]\n",
    "    if(len(sentences) > 0):\n",
    "      result_dict[key] = sentences\n",
    "  return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione che restituisce la valutazione fatta per span comprensiva di tipo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Questa funzione si occupa di calcolare le metriche di precision, recall e f1 score per le entità estratte dal modello\n",
    "\n",
    "def scores_span(path_result,path_dataset):\n",
    "\n",
    "  def evaluate_ner(ground_truth, predicted):\n",
    "      # Get unique entity types\n",
    "      entity_types = set(ground_truth.keys()).union(predicted.keys())\n",
    "\n",
    "      # Inizializzo le variabili per i true positives, false positives e false negatives\n",
    "      true_positives = 0\n",
    "      false_positives = 0\n",
    "      false_negatives = 0\n",
    "       \n",
    "      # Per ogni tipo di entità calcolo i true positives, false positives e false negatives\n",
    "\n",
    "      for entity_type in entity_types:\n",
    "          true_positives += sum(1 for entity in ground_truth.get(entity_type, [])\n",
    "                                if entity in predicted.get(entity_type, []))\n",
    "        \n",
    "          false_positives += sum(1 for entity in predicted.get(entity_type, [])\n",
    "                                if entity not in ground_truth.get(entity_type, []))\n",
    "\n",
    "          false_negatives += sum(1 for entity in ground_truth.get(entity_type, [])\n",
    "                                if entity not in predicted.get(entity_type, []))\n",
    "\n",
    "      return true_positives, false_positives, false_negatives\n",
    "\n",
    "  true_positives = 0\n",
    "  false_positives = 0\n",
    "  false_negatives = 0\n",
    "  i = 0\n",
    "  data = []\n",
    "  \n",
    "  # Salvo le entità estratte dal modello che erano presenti nel file delle predizioni\n",
    "  with open(path_result) as f:\n",
    "      for line in f:\n",
    "          data.append(json.loads(line)[\"prediction\"])\n",
    "  # Per ogni riga del dataset calcolo le metriche\n",
    "  with open(path_dataset) as f:\n",
    "      for line in f:\n",
    "          data2 = json.loads(line)\n",
    "          result = data[i]\n",
    "          predicted_output = extract_entities_with_categories_from_prediction(result)\n",
    "          result_dict = extract_entities_with_categories_from_data(data2)\n",
    "          aux1, aux2, aux3 = evaluate_ner(result_dict,predicted_output)\n",
    "          true_positives += aux1\n",
    "          false_positives += aux2\n",
    "          false_negatives += aux3\n",
    "          i += 1\n",
    "\n",
    "  precision = true_positives / (true_positives + false_positives)\n",
    "  recall = true_positives / (true_positives + false_negatives)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  print(\"Precision: \" + \"{:.2f}\".format(precision * 100) + \"%\")\n",
    "  print(\"Recall: \" + \"{:.2f}\".format(recall * 100) + \"%\")\n",
    "  print(\"F1 Score: \" + \"{:.2f}\".format(f1_score * 100) + \"%\")\n",
    "\n",
    "# Esempio di utilizzo, in questo caso vengono calcolate le metriche per tutti i tipi di dataset\n",
    "#print(\"\\nNo incapsulamento\")\n",
    "#scores_span(\"/content/drive/MyDrive/Tesi/result/result_flan_no_inc_15.json\",\"/content/drive/MyDrive/Tesi/dataset/scierc_test_no_inc.json\")\n",
    "#print(\"\\nCon incapsulamento\")\n",
    "#scores_span(\"/content/drive/MyDrive/Tesi/result/result_flan_inc_15.json\",\"/content/drive/MyDrive/Tesi/dataset/scierc_test_inc.json\")\n",
    "#print(\"\\nNo differenze\")\n",
    "#scores_span(\"/content/drive/MyDrive/Tesi/result/result_flan_no_diff_15.json\",\"/content/drive/MyDrive/Tesi/dataset/scierc_test_no_diff.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
