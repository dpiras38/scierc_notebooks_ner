{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzioni che sono necessarie per la corretta estrazione e organizzazione delle risposte del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Questa funzione si occupa di ottenere le entità estratte dal modello e di organizzarle in un dizionario\n",
    "\n",
    "def extract_entities_with_categories_from_prediction(sentence):\n",
    "  pattern = re.compile(r'(\\w+):\\s*([^;]+)')\n",
    "  matches = pattern.findall(sentence)\n",
    "  keys = [\"Metric\", \"Method\", \"Material\", \"Task\", \"OtherScientificTerm\", \"Generic\"]\n",
    "  result_dict = {}\n",
    "  for match in matches:\n",
    "      key = match[0].strip()\n",
    "      values = match[1].strip()\n",
    "      if(key in result_dict.keys()):\n",
    "        result_dict[key].append(values)\n",
    "      else:\n",
    "        result_dict[key] = []\n",
    "        result_dict[key].append(values)\n",
    "  return result_dict\n",
    "\n",
    "# Questa funzione si occupa di organizzare le entità che sono presenti nel dataset in un dizionario\n",
    "def extract_entities_with_categories_from_data(data):\n",
    "  keys = [\"Metric\", \"Method\", \"Material\", \"Task\", \"OtherScientificTerm\", \"Generic\"]\n",
    "  result_dict = {}\n",
    "  for key in keys:\n",
    "    sentences = data[key]\n",
    "    if(len(sentences) > 0):\n",
    "      result_dict[key] = sentences\n",
    "  return result_dict\n",
    "\n",
    "# Questa funzione si occupa di mappare le span delle entità token per token\n",
    "\n",
    "def map_ner_to_sentence(ner_output, original_sentence):\n",
    "    tokens = original_sentence\n",
    "\n",
    "    ner_labels = ['O'] * len(tokens)\n",
    "\n",
    "    for category, entities in ner_output.items():\n",
    "        for entity in entities:\n",
    "            entity_tokens = word_tokenize(entity)\n",
    "            entity_length = len(entity_tokens)\n",
    "            for i in range(len(tokens) - entity_length + 1):\n",
    "                if tokens[i:i + entity_length] == entity_tokens:\n",
    "                    ner_labels[i:i + entity_length] = [category] * (entity_length) \n",
    "                    break  \n",
    "    return ner_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione che restituisce la valutazione fatta per span comprensiva di tipo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Questa funzione si occupa di calcolare le metriche di precision, recall e f1 score per le entità estratte dal modello\n",
    "\n",
    "def scores_span(path_result,path_dataset):\n",
    "\n",
    "  def evaluate_ner(ground_truth, predicted):\n",
    "      # Get unique entity types\n",
    "      entity_types = set(ground_truth.keys()).union(predicted.keys())\n",
    "\n",
    "      # Inizializzo le variabili per i true positives, false positives e false negatives\n",
    "      true_positives = 0\n",
    "      false_positives = 0\n",
    "      false_negatives = 0\n",
    "       \n",
    "      # Per ogni tipo di entità calcolo i true positives, false positives e false negatives\n",
    "\n",
    "      for entity_type in entity_types:\n",
    "          true_positives += sum(1 for entity in ground_truth.get(entity_type, [])\n",
    "                                if entity in predicted.get(entity_type, []))\n",
    "        \n",
    "          false_positives += sum(1 for entity in predicted.get(entity_type, [])\n",
    "                                if entity not in ground_truth.get(entity_type, []))\n",
    "\n",
    "          false_negatives += sum(1 for entity in ground_truth.get(entity_type, [])\n",
    "                                if entity not in predicted.get(entity_type, []))\n",
    "\n",
    "      return true_positives, false_positives, false_negatives\n",
    "\n",
    "  true_positives = 0\n",
    "  false_positives = 0\n",
    "  false_negatives = 0\n",
    "  i = 0\n",
    "  data = []\n",
    "  \n",
    "  # Salvo le entità estratte dal modello che erano presenti nel file delle predizioni\n",
    "  with open(path_result) as f:\n",
    "      for line in f:\n",
    "          data.append(json.loads(line)[\"prediction\"])\n",
    "  # Per ogni riga del dataset calcolo le metriche\n",
    "  with open(path_dataset) as f:\n",
    "      for line in f:\n",
    "          data2 = json.loads(line)\n",
    "          result = data[i]\n",
    "          predicted_output = extract_entities_with_categories_from_prediction(result)\n",
    "          result_dict = extract_entities_with_categories_from_data(data2)\n",
    "          aux1, aux2, aux3 = evaluate_ner(result_dict,predicted_output)\n",
    "          true_positives += aux1\n",
    "          false_positives += aux2\n",
    "          false_negatives += aux3\n",
    "          i += 1\n",
    "\n",
    "  precision = true_positives / (true_positives + false_positives)\n",
    "  recall = true_positives / (true_positives + false_negatives)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  print(\"Precision: \" + \"{:.2f}\".format(precision * 100) + \"%\")\n",
    "  print(\"Recall: \" + \"{:.2f}\".format(recall * 100) + \"%\")\n",
    "  print(\"F1 Score: \" + \"{:.2f}\".format(f1_score * 100) + \"%\")\n",
    "\n",
    "# Esempio di utilizzo, in questo caso vengono calcolate le metriche per tutti i tipi di dataset\n",
    "#print(\"\\nNo incapsulamento\")\n",
    "#scores_span(\"/content/drive/MyDrive/Tesi/result/result_flan_no_inc_15.json\",\"/content/drive/MyDrive/Tesi/dataset/scierc_test_no_inc.json\")\n",
    "#print(\"\\nCon incapsulamento\")\n",
    "#scores_span(\"/content/drive/MyDrive/Tesi/result/result_flan_inc_15.json\",\"/content/drive/MyDrive/Tesi/dataset/scierc_test_inc.json\")\n",
    "#print(\"\\nNo differenze\")\n",
    "#scores_span(\"/content/drive/MyDrive/Tesi/result/result_flan_no_diff_15.json\",\"/content/drive/MyDrive/Tesi/dataset/scierc_test_no_diff.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione che restituisce la valutazione fatta per span senza considerare il tipo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Questa funzione si occupa di calcolare le metriche di precision, recall e f1 score per le entità estratte dal modello usando\n",
    "# un approccio che non considera i tipi delle entità\n",
    "\n",
    "def scores_span_no_type(path_result,path_dataset):\n",
    "\n",
    "  def evaluate_ner(ground_truth, predicted):\n",
    "      # Get unique entity types\n",
    "      entity_types = set(ground_truth.keys()).union(predicted.keys())\n",
    "      gt_list = []\n",
    "      prediction_list = []\n",
    "      merged_list = []\n",
    "\n",
    "      for entity_type in entity_types:\n",
    "        if(entity_type in ground_truth.keys()):\n",
    "          for element in ground_truth[entity_type]:\n",
    "            gt_list.append(element)\n",
    "            if(element not in merged_list):\n",
    "              merged_list.append(element)\n",
    "        if(entity_type in predicted.keys()):\n",
    "          for element in predicted[entity_type]:\n",
    "            prediction_list.append(element)\n",
    "            if(element not in merged_list):\n",
    "              merged_list.append(element)\n",
    "\n",
    "      true_positives = 0\n",
    "      false_positives = 0\n",
    "      false_negatives = 0\n",
    "\n",
    "      for element in merged_list:\n",
    "          if(element in gt_list and element in prediction_list):\n",
    "            true_positives += 1\n",
    "\n",
    "          if(element not in gt_list and element in prediction_list):\n",
    "            false_positives += 1\n",
    "\n",
    "          if(element in gt_list and element not in prediction_list):\n",
    "            false_negatives += 1\n",
    "\n",
    "      return true_positives, false_positives, false_negatives\n",
    "\n",
    "  true_positives = 0\n",
    "  false_positives = 0\n",
    "  false_negatives = 0\n",
    "  i = 0\n",
    "  data = []\n",
    "\n",
    "  # Salvo le entità estratte dal modello che erano presenti nel file delle predizioni\n",
    "\n",
    "  with open(path_result) as f:\n",
    "      for line in f:\n",
    "          data.append(json.loads(line)[\"prediction\"])\n",
    "\n",
    "  # Per ogni riga del dataset calcolo le metriche\n",
    "\n",
    "  with open(path_dataset) as f:\n",
    "      for line in f:\n",
    "          data2 = json.loads(line)\n",
    "          result = data[i]\n",
    "          predicted_output = extract_entities_with_categories_from_prediction(result)\n",
    "          result_dict = extract_entities_with_categories_from_data(data2)\n",
    "          aux1, aux2, aux3 = evaluate_ner(result_dict,predicted_output)\n",
    "          true_positives += aux1\n",
    "          false_positives += aux2\n",
    "          false_negatives += aux3\n",
    "          i += 1\n",
    "\n",
    "  precision = true_positives / (true_positives + false_positives)\n",
    "  recall = true_positives / (true_positives + false_negatives)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  print(\"Precision: \" + \"{:.2f}\".format(precision * 100) + \"%\")\n",
    "  print(\"Recall: \" + \"{:.2f}\".format(recall * 100) + \"%\")\n",
    "  print(\"F1 Score: \" + \"{:.2f}\".format(f1_score * 100) + \"%\")\n",
    "\n",
    "# Esempio di utilizzo, in questo caso vengono calcolate le metriche per tutti i tipi di dataset\n",
    "\n",
    "#print(\"\\nNo incapsulamento\")\n",
    "#scores_span_no_type(\"/content/drive/MyDrive/Tesi/result/result_flan_no_inc_15.json\",\"/content/drive/MyDrive/Tesi/dataset/scierc_test_no_inc.json\")\n",
    "#print(\"\\nCon incapsulamento\")\n",
    "#scores_span_no_type(\"/content/drive/MyDrive/Tesi/result/result_flan_inc_15.json\",\"/content/drive/MyDrive/Tesi/dataset/scierc_test_inc.json\")\n",
    "#print(\"\\nNo differenze\")\n",
    "#scores_span_no_type(\"result_flan_large_no_diff.json\",\"/content/drive/MyDrive/Tesi/dataset/scierc_test_no_diff.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
