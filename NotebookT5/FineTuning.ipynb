{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYV20gc1X8e7"
      },
      "source": [
        "Installazione dipendenze\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPWlTtSvXv5S"
      },
      "outputs": [],
      "source": [
        "%pip install nltk\n",
        "%pip install datasets\n",
        "%pip install transformers[torch]\n",
        "%pip install tokenizers\n",
        "%pip install evaluate\n",
        "%pip install rouge_score\n",
        "%pip install sentencepiece\n",
        "%pip install huggingface_hub\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlBYj7IaX_qY"
      },
      "source": [
        "Fine Tuning Flan T5 senza essere quantizzato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nzAhvTJX7F5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Scegliamo il modello che vogliamo usare e carichiamo il tokenizer e il modello\n",
        "MODEL_NAME = \"google/flan-t5-large\"\n",
        "tokenizer = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "model = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# Path del training e del validation set\n",
        "path_train = '/content/drive/MyDrive/Tesi/dataset/scierc_train_no_diff.json'\n",
        "path_val = '/content/drive/MyDrive/Tesi/dataset/scierc_dev_no_diff.json'\n",
        "\n",
        "# Carichiamo il dataset, train e validation\n",
        "train = load_dataset('json', data_files=path_train, split='train')\n",
        "val = load_dataset('json', data_files=path_val, split='train')\n",
        "\n",
        "\n",
        "# Funzione di preprocessing\n",
        "\n",
        "def preprocess_function(examples):\n",
        "   # L'input Ã¨ formato da prefix e dalla frase di input\n",
        "   # Prefix\n",
        "   prefix = \"NER: \"\n",
        "   inputs = f\"{prefix}: {examples['input']}\"\n",
        "   model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "\n",
        "   # Viene definito l'output\n",
        "   keys = [\"Metric\", \"Method\", \"Material\", \"Task\", \"OtherScientificTerm\", \"Generic\"]\n",
        "   text = \"\"\n",
        "   for key in keys:\n",
        "    for elem in examples[key]:\n",
        "      text += f\"\"\"{key}: {elem}; \"\"\"\n",
        "\n",
        "\n",
        "   output = text[:-1]\n",
        "   labels = tokenizer(text_target=output,\n",
        "                      max_length=512,\n",
        "                      truncation=True)\n",
        "\n",
        "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "   return model_inputs\n",
        "\n",
        "# Viene applicata la funzione di preprocessing ad entrambi i dataset\n",
        "tokenized_dataset = train.map(preprocess_function)\n",
        "tokenized_dataset2 = val.map(preprocess_function)\n",
        "\n",
        "# Parametri globali\n",
        "L_RATE = 3e-4\n",
        "BATCH_SIZE = 16\n",
        "PER_DEVICE_EVAL_BATCH = 8\n",
        "WEIGHT_DECAY = 0.01\n",
        "SAVE_TOTAL_LIM = 2\n",
        "NUM_EPOCHS = 15\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/Tesi/models/flan-t5-large-ner-no-diff-10\"\n",
        "\n",
        "# Prepariamo i parametri del training\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "   output_dir = output_dir,\n",
        "   evaluation_strategy=\"epoch\",\n",
        "   learning_rate=L_RATE,\n",
        "   per_device_train_batch_size=BATCH_SIZE,\n",
        "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
        "   weight_decay=WEIGHT_DECAY,\n",
        "   save_total_limit=SAVE_TOTAL_LIM,\n",
        "   num_train_epochs=NUM_EPOCHS,\n",
        "   predict_with_generate=True,\n",
        "   push_to_hub=False,\n",
        "   save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_dataset,\n",
        "   eval_dataset=tokenized_dataset2,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Avviamo il fine tuning\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8zq7gh_YGHR"
      },
      "source": [
        "Generazione risposte nel file JSON"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
