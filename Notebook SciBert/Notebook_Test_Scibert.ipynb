{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLMxynz-Cfxr"
      },
      "source": [
        "\n",
        "- Installazione dipendenze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5Ej2nbj_lAh"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "\n",
        "!pip install datasets\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aNYpTCxD_W08"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
        "import numpy as np\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score\n",
        "from transformers import AutoModelForTokenClassification\n",
        "import torch\n",
        "from transformers import BertForTokenClassification, BertTokenizer\n",
        "import datasets\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhDkkodSvRQg"
      },
      "source": [
        "- Definizione del modello e del tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8Ghagh2Q_sKt"
      },
      "outputs": [],
      "source": [
        "model_path=\"/content/drive/MyDrive/Tesi/Model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j2J_2rovaye"
      },
      "source": [
        "- Definizione lista delle label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l7Pr59LW_vNt"
      },
      "outputs": [],
      "source": [
        "label_list=[\"O\",\"B-Task\",\"I-Task\",\"B-Method\",\"I-Method\",\"B-Metric\",\"I-Metric\",\"B-Material\",\"I-Material\",\"B-OtherScientificTerm\",\"I-OtherScientificTerm\",\"B-Generic\",\"I-Generic\"]\n",
        "\n",
        "\n",
        "label_map = {label: i for i, label in enumerate(label_list)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbcxzzpDRf_U"
      },
      "source": [
        "- Caricamento dataset e successiva tokenizzazione, può essere bypassato se si dispone del dataset già tokenizzato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QqjTxa9_xct"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset('json', data_files='/content/drive/MyDrive/Tesi/dataset/scierc_train_ner_with_inc.json', split='train')\n",
        "val_dataset = load_dataset('json', data_files='/content/drive/MyDrive/Tesi/dataset/scierc_dev_ner_with_inc.json', split='train')\n",
        "test_dataset= load_dataset('json', data_files='/content/drive/MyDrive/Tesi/dataset/scierc_test_ner_with_inc.json', split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ltQNmLr_1Hq"
      },
      "outputs": [],
      "source": [
        "datasets = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    \"test\": test_dataset,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_YzW6tD_5eV"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels_IB(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"sentence\"],truncation=True, is_split_into_words=True,\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        tokens= tokenized_inputs.tokens(batch_index=i)\n",
        "        label_ids = []\n",
        "        previous_word_id=None\n",
        "\n",
        "        for word_idx,token in zip(word_ids,tokens):\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif (word_idx != previous_word_id and label[word_idx]!=\"O\" ):\n",
        "                \n",
        "                label_ids.append(label_map[\"B-\"+label[word_idx]])\n",
        "                previous_word_id=word_idx\n",
        "            elif (label[word_idx]!=\"O\"):\n",
        "                \n",
        "                label_ids.append(label_map[\"I-\"+label[word_idx]])\n",
        "            else:\n",
        "                \n",
        "                label_ids.append(label_map[label[word_idx]])\n",
        "\n",
        "\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed4WRfNM_8BG"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = datasets.map(tokenize_and_align_labels_IB, batched=True,remove_columns=datasets[\"train\"].column_names,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Wnpw6_sOAZ3b"
      },
      "outputs": [],
      "source": [
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Creazione oggetto model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4sQio6OW_8oY"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_path,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ussPpnHeRsaJ"
      },
      "source": [
        "- Caricamento dataset tokenizzato dalla memoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "51PZCHJnFp_k"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets=datasets.load_from_disk(\"/content/drive/MyDrive/Tesi/dataset/dataset_tokenizzati\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVtDgXf4SFuJ"
      },
      "source": [
        "- Generazione risposte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZrkMK35AdaT"
      },
      "outputs": [],
      "source": [
        "results=[]\n",
        "cls_token_id = tokenizer.cls_token_id  # ID of [CLS] token\n",
        "sep_token_id = tokenizer.sep_token_id  # ID of [SEP] token\n",
        "\n",
        "for i,sentence in enumerate(tokenized_datasets[\"test\"][\"input_ids\"]):\n",
        "\n",
        "    #print(i,\"\\n--------------------------------------\")\n",
        "\n",
        "    input_ids = [x for x in sentence if x != cls_token_id and x != sep_token_id]\n",
        "    #print(tokenizer.decode(input_ids))\n",
        "    with torch.no_grad():\n",
        "\n",
        "\n",
        "        input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
        "\n",
        "        outputs = model(input_ids=input_ids)\n",
        "        logits = outputs.logits\n",
        "        predicted_label_ids = torch.argmax(logits, dim=-1).squeeze(0).tolist()\n",
        "\n",
        "        #predicted_labels=[ label_list[x] for x in predicted_label_ids if x!=-100]\n",
        "\n",
        "        results.append(predicted_label_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0v2ca-QR-x4"
      },
      "source": [
        "- Salvataggio predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "09l8_Hw9CpvN"
      },
      "outputs": [],
      "source": [
        "file_risultati=\"/content/drive/MyDrive/Tesi/Model/result/result_predictions_with_inc.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gAf42PTACr4N"
      },
      "outputs": [],
      "source": [
        "with open(file_risultati, 'a') as f:\n",
        "    json.dump({\"predictions_id\":results}, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
