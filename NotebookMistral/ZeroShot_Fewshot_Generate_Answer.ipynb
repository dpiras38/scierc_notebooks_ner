{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuwBDUvU_bjc"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers peft accelerate optimum\n",
        "!pip install bitsandbytes\n",
        "!pip install auto-gptq\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "a6BLsnR5tKYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "startTime = time.time()\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "n = 1\n",
        "#scegliere se usare Mistral7b o LLama27b\n",
        "model_name_or_path = \"TheBloke/Llama-2-7B-GPTQ\"#Mistral-7B-v0.1-GPTQ\"#\n",
        "path = \"/content/drive/MyDrive/Tesi/results.json\"\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Tesi/dataset/scierc_test_ner_both.json\") as f:\n",
        "    for line in f:\n",
        "\n",
        "\n",
        "        data = json.loads(line)\n",
        "        question = data[\"input\"]\n",
        "\n",
        "        prompt_zero = f\"\"\"### Question: Identify and extract without rephrasing all entities that belongs to the categories Method, Metric, Material, Task, OtherScientificTerm, Generic from the following sentence in a list format like \"Category: Entity\": \"{question}\"\\n\n",
        "        These are the annotation guideline to extract entities:\n",
        "        Task are Applications, problems to solve, systems to construct;\n",
        "        Method are Methods , models, systems to use, or tools, components of a system;\n",
        "        Evaluation Metric are Metrics, measures, or entities that can express quality of a system/method;\n",
        "        Material are Data, datasets, resources, Corpus, Knowledge base;\n",
        "        OtherScientificTerms are Phrases that are a scientific terms but do not fall into any of the above classes;\n",
        "        Generic are General terms or pronouns that may refer to a entity but are not themselves informative.\n",
        "        ### Answer:\"\"\"\n",
        "\n",
        "        prompt_one =f\"\"\" ### Question: Identify and extract, without rephrasing, all entities that belong to the categories Method, Metric, Material, Task, OtherScientificTerm, and Generic from the test sentence.\n",
        "        These are the annotation guidelines to extract entities:\n",
        "        - *Task:* Applications, problems to solve, systems to construct.\n",
        "        - *Method:* Methods, models, systems to use, or tools, components of a system.\n",
        "        - *Evaluation Metric:* Metrics, measures, or entities that can express the quality of a system/method.\n",
        "        - *Material:* Data, datasets, resources, Corpus, Knowledge base.\n",
        "        - *OtherScientificTerms:* Phrases that are scientific terms but do not fall into any of the above classes.\n",
        "        - *Generic:* General terms or pronouns that may refer to an entity but are not themselves informative.\n",
        "\n",
        "        Below are some examples:\n",
        "\n",
        "        Input Sentence: \"We show results from multi-modal super-resolution and face recognition experiments across different imaging modalities , using low-resolution images as testing inputs and demonstrate improved recognition rates over standard tensorface and eigenface representations\"\n",
        "        Output Sentence: Task: multi-modal super-resolution and face recognition; OtherScientificTerm: imaging modalities; Material: low-resolution images; Metric: recognition rates; Method: tensorface and eigenface representations;\n",
        "\n",
        "        Test Sentence:\"{question}\"\n",
        "        ### Answer:\"\"\"\n",
        "\n",
        "        prompt_three =f\"\"\" ### Question: Identify and extract, without rephrasing, all entities that belong to the categories Method, Metric, Material, Task, OtherScientificTerm, and Generic from the test sentence.\n",
        "\n",
        "        These are the annotation guidelines to extract entities:\n",
        "        - *Task:* Applications, problems to solve, systems to construct.\n",
        "        - *Method:* Methods, models, systems to use, or tools, components of a system.\n",
        "        - *Evaluation Metric:* Metrics, measures, or entities that can express the quality of a system/method.\n",
        "        - *Material:* Data, datasets, resources, Corpus, Knowledge base.\n",
        "        - *OtherScientificTerms:* Phrases that are scientific terms but do not fall into any of the above classes.\n",
        "        - *Generic:* General terms or pronouns that may refer to an entity but are not themselves informative.\n",
        "\n",
        "        Below are some examples:\n",
        "\n",
        "        Input Sentence: \"The recognition quality is evaluated through retrieval on a database with ground truth , showing the power of the vocabulary tree approach , going as high as 1 million images\"\n",
        "        Output Sentence: Metric: recognition quality; Task: retrieval; Material: database with ground truth; Method: vocabulary tree approach;\n",
        "\n",
        "        Input Sentence: \"We perform extensive statistical analyses to compare our dataset to existing image and video description datasets\"\n",
        "        Output Sentence: OtherScientificTerm: statistical analyses; Generic: dataset; Material: image and video description datasets;\n",
        "\n",
        "        Input Sentence: \"In experimental evaluation , our proposed method outperforms previous shift-reduce dependency parsers for the Chine language , showing improvement of dependency accuracy by 10.08 %\"\n",
        "        Output Sentence: Generic: method; Method: shift-reduce dependency parsers; Material: Chine language; Metric: dependency accuracy;\n",
        "\n",
        "        Test Sentence:\"{question}\"\n",
        "        ### Answer:\"\"\"\n",
        "\n",
        "        #cambiare il tipo di prompt all'interno del tokenizer a seconda dell'esperimento\n",
        "        input_ids = tokenizer(eval_prompt, return_tensors='pt').input_ids.cuda()\n",
        "\n",
        "        output = model.generate(inputs=input_ids,do_sample=False,max_new_tokens=70)\n",
        "\n",
        "        result=tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        print (result)\n",
        "        print(n,\"Time taken: \", time.time() -startTime,)\n",
        "\n",
        "        with open(path, 'a') as outfile:\n",
        "          json_object = {\"prediction\": result}\n",
        "          json.dump(json_object, outfile)\n",
        "          outfile.write('\\n')\n",
        "        print(n)\n",
        "        n += 1\n"
      ],
      "metadata": {
        "id": "l-JEVyZDKYQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities_with_categories_from_prediction(sentence):\n",
        "  lista_Keyword=[\"Method\",\"Task\",\"Metric\",\"Material\",\"OtherScientificTerm\",\"Generic\"]\n",
        "  pattern = re.compile(r'Answer:\\s*([^,]+(?:,\\s*[^,]+)*)')\n",
        "\n",
        "  matches = pattern.findall(sentence)\n",
        "  #print(matches, \"   match1 \")\n",
        "\n",
        "  pattern = re.compile(r'(\\w+):\\s*([^;\\n]+|$)')\n",
        "  #pattern = re.compile(r'(\\w+):\\s*([^\\n]+)')\n",
        "  matches = pattern.findall(matches[0])\n",
        "  #print(matches, \"   match2 \")\n",
        "\n",
        "  result_dict = {}\n",
        "  for match in matches:\n",
        "\n",
        "      key = match[0].strip()\n",
        "      #print(key,\"  key\")\n",
        "      values = [value.strip() for value in match[1].split(',')]\n",
        "\n",
        "      #print(values),\"  values\"\n",
        "      if(key in lista_Keyword):\n",
        "        if(len(values) > 1 ):\n",
        "          if( key in result_dict.keys() ):\n",
        "            for v in values:\n",
        "              result_dict[key].append(v)\n",
        "          else:\n",
        "            result_dict[key] = values\n",
        "        else:\n",
        "          if(values[0] != ''):\n",
        "            if(key in result_dict.keys()):\n",
        "              result_dict[key].append(values[0])\n",
        "            else:\n",
        "              result_dict[key] = values\n",
        "\n",
        "  return result_dict\n",
        "\n",
        "\n",
        "def extract_entities_with_categories_from_data(data):\n",
        "  keys = [\"Metric\", \"Method\", \"Material\", \"Task\", \"OtherScientificTerm\", \"Generic\"]\n",
        "  result_dict = {}\n",
        "  for key in keys:\n",
        "    sentence = data[key]\n",
        "    if(sentence != \"\"):\n",
        "      result_dict[key] = sentence.split(\", \")\n",
        "  return result_dict\n",
        "\n",
        "def map_ner_to_sentence(ner_output, original_sentence):\n",
        "    # Tokenize the sentence\n",
        "    tokens = original_sentence\n",
        "\n",
        "    # Initialize NER labels using IOB format\n",
        "    ner_labels = ['O'] * len(tokens)\n",
        "\n",
        "    # Map NER labels based on the provided information\n",
        "    for category, entities in ner_output.items():\n",
        "        for entity in entities:\n",
        "            entity_tokens = word_tokenize(entity)\n",
        "            entity_length = len(entity_tokens)\n",
        "            for i in range(len(tokens) - entity_length + 1):\n",
        "                if tokens[i:i + entity_length] == entity_tokens:\n",
        "                    # Set NER labels for the identified entity tokens using IOB format\n",
        "                    ner_labels[i:i + entity_length] = [category] * (entity_length)  # Inside of entity\n",
        "                    break  # Exit the loop once an entity is found\n",
        "    return ner_labels"
      ],
      "metadata": {
        "id": "JuCuarhiNvKM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}